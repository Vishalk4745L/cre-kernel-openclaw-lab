ðŸ” CRE Kernel Lab â€” Trust Model Design



> Trust is not confidence.

Trust is not accuracy.

Trust is earned, tracked, and revocable.









---



1\. Why a Trust Model Exists



Modern AI systems fail not because they are weak â€”

but because they believe themselves too easily.



Problems we must solve:



Context rot



Silent hallucinations



Long-term drift



Agent overconfidence



Memory poisoning





The Trust Model exists to answer one question:



> â€œShould this response be believed?â€









---



2\. Trust vs Confidence (Critical Distinction)



Concept	Meaning



Confidence	How sure an agent claims to be

Trust	How reliable the agent has proven over time





> Confidence is declared.

Trust is measured.







An agent may be:



High confidence, low trust



Low confidence, high trust



Temporarily trusted, permanently distrusted







---



3\. Design Principles



The Trust Model must be:



âœ… Deterministic

âœ… Auditable

âœ… Agent-agnostic

âœ… Model-agnostic

âœ… Time-aware

âœ… Revocable



It must never:



Auto-correct itself



Self-forgive errors



Trust blindly







---



4\. Trust Model Scope (Lab Version)



In the Kernel Lab (Open Source):



Trust is computed



Trust is tracked



Trust is reported





But trust is NOT enforced.



> Enforcement lives in the private Kernel Core.







This keeps the Lab:



Safe



Non-authoritative



Research-friendly







---



5\. Trust Entity Model



Each agent has an associated trust record.



{

&nbsp; "agent\_id": "openclaw-senior",

&nbsp; "trust\_score": 0.72,

&nbsp; "observations": 148,

&nbsp; "last\_updated": "2026-02-10T10:45:00Z"

}





---



6\. Trust Score Definition



Trust score is a bounded scalar:



0.0 â‰¤ trust â‰¤ 1.0



Interpretation:



Score	Meaning



0.0	Never trust

0.3	Highly unreliable

0.5	Neutral

0.7	Reliable

0.9	Highly trusted





Trust never starts at 1.0.





---



7\. How Trust Is Updated



Trust changes only after resolution events.



Inputs:



Agent reply



Resolver outcome



Agreement / disagreement



Confidence alignment



Failure mode (timeout, error, abstain)







---



Conceptual Update Formula



trust\_new = trust\_old

&nbsp;          + Î± \* correctness

&nbsp;          - Î² \* contradiction

&nbsp;          - Î³ \* failure



Where:



Î±, Î², Î³ are kernel-controlled weights



Lab version uses fixed constants







---



8\. Trust Is Time-Aware



Trust decays without reinforcement.



Why?



Models drift



Tools change



Context shifts





Conceptually:



trust(t) = trust(tâ‚€) \* e^(âˆ’Î»Î”t)



No recent success â†’ trust slowly fades.





---



9\. Trust vs Consensus



Consensus â‰  Truth.



Resolver behavior:



Consensus influences final reply



Trust influences future weighting





Example:



Agent A agrees often â†’ trust â†‘



Agent B disagrees but is correct â†’ trust â†‘



Agent C agrees blindly â†’ trust â†“







---



10\. Trust Does NOT Mean Authority



Even a high-trust agent:



Can be ignored



Can be overridden



Can be sandboxed





> No agent is ever sovereign.









---



11\. Failure Modes Tracked by Trust



Trust is penalized for:



âŒ Confidently wrong answers

âŒ Repeated hallucinations

âŒ Tool misuse

âŒ Silent failures

âŒ Refusal loops



Trust is not penalized for:



Abstaining



Expressing uncertainty



Requesting clarification







---



12\. Trust and Memory Safety



Critical rule:



> Memory writes are gated by trust.







In Lab:



Trust is logged only





In Core (private):



Trust gates:



Long-term memory writes



Belief persistence



Cross-session propagation









---



13\. Trust Prevents Context Poisoning



Attack scenario:



1\. Agent injects false claim





2\. Claim propagates into memory





3\. Future agents inherit falsehood







Trust model stops this by:



Penalizing contradictions



Down-weighting repeat offenders



Requiring multi-agent agreement







---



14\. Trust and Infinite Agents



The system supports N agents.



Trust model properties:



O(N) tracking



Independent scores



No pairwise explosion



No graph cycles (lab version)







---



15\. Trust Is Observable



Every resolution exposes trust metadata:



{

&nbsp; "agent": "openclaw-junior",

&nbsp; "reply": "...",

&nbsp; "confidence": 0.6,

&nbsp; "trust": 0.71

}



This allows:



Debugging



Research



Visualization



Governance







---



16\. Trust Is Reversible



Trust is never permanent.



A previously trusted agent can:



Decay



Fall



Be quarantined





This avoids:



Institutional bias



Legacy dominance



Model lock-in







---



17\. Trust vs Alignment



Trust does not mean:



Moral correctness



Policy alignment



Ethical approval





Trust answers only:



> â€œDoes this agent tend to be right?â€









---



18\. Why Trust Lives Outside Agents



Agents must not:



See their trust score



Adjust behavior to game it



Compete for trust





Trust is:



External



Observational



Non-interactive







---



19\. Trust Model Limitations (Lab)



Known limits:



No self-repair



No causal reasoning



No belief graphs



No meta-trust





These are intentional.





---



20\. Summary



> Confidence is what agents say.

Trust is what the system remembers.







The Trust Model:



Protects memory



Stabilizes truth



Enables scale



Prevents silent failure







---



21\. One-Line Definition



> Trust is delayed confidence, earned through survival.









---



ðŸ§  End of Trust Model Design (Kernel Lab)





---





